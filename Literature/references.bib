@Article{Pas2022,
  author    = {Pieter Pas and Mathijs Schuurmans and Panagiotis Patrinos},
  title     = {Alpaqa: A matrix-free solver for nonlinear {MPC} and large-scale nonconvex optimization},
  year      = {2022},
  month     = {jul},
  booktitle = {2022 European Control Conference ({ECC})},
  doi       = {10.23919/ecc55457.2022.9838172},
  groups    = {ALM-PANOC},
  publisher = {{IEEE}},
  ranking   = {rank5},
}

@Article{Bodard2023,
  author    = {Alexander Bodard and Pieter Pas and Panagiotis Patrinos},
  journal   = {{IEEE} Control Systems Letters},
  title     = {{PANTR}: A Proximal Algorithm With Trust-Region Updates for Nonconvex Constrained Optimization},
  year      = {2023},
  pages     = {2389--2394},
  volume    = {7},
  doi       = {10.1109/lcsys.2023.3286331},
  groups    = {ALM-PANOC},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  ranking   = {rank5},
}

@Article{Pas2022a,
  author    = {Pas, Pieter and Themelis, Andreas and Patrinos, Panagiotis},
  title     = {Gauss-Newton meets PANOC: A fast and globally convergent algorithm for nonlinear optimal control},
  year      = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.2212.04391},
  groups    = {ALM-PANOC},
  keywords  = {Optimization and Control (math.OC), Systems and Control (eess.SY), FOS: Mathematics, FOS: Mathematics, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, 65K05, 49M37, 90C30},
  publisher = {arXiv},
  ranking   = {rank4},
}

@Article{Li2016,
  author    = {Li, Ke and Malik, Jitendra},
  title     = {Learning to Optimize},
  year      = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1606.01885},
  groups    = {General},
  keywords  = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Optimization and Control (math.OC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  publisher = {arXiv},
  ranking   = {rank5},
}

@Article{Reijnen2022,
  author    = {Robbert Reijnen and Yingqian Zhang and Zaharah Bukhsh and Mateusz Guzek},
  title     = {Deep Reinforcement Learning for Adaptive Parameter Control in Differential Evolution for Multi-Objective Optimization},
  year      = {2022},
  month     = {dec},
  booktitle = {2022 {IEEE} Symposium Series on Computational Intelligence ({SSCI})},
  doi       = {10.1109/ssci51031.2022.10022227},
  groups    = {Proximal Policy Optimization},
  publisher = {{IEEE}},
  ranking   = {rank3},
}

@Article{Kozlica2023,
  author    = {Reuf Kozlica and Stefan Wegenkittl and Simon Hiränder},
  title     = {Deep Q-Learning versus Proximal Policy Optimization: Performance Comparison in a Material Sorting Task},
  year      = {2023},
  month     = {jun},
  booktitle = {2023 {IEEE} 32nd International Symposium on Industrial Electronics ({ISIE})},
  doi       = {10.1109/isie51358.2023.10228056},
  groups    = {Proximal Policy Optimization},
  publisher = {{IEEE}},
  ranking   = {rank2},
}

@Article{Liu2019,
  author    = {Liu, Boyi and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
  title     = {Neural Proximal/Trust Region Policy Optimization Attains Globally Optimal Policy},
  year      = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1906.10306},
  groups    = {Proximal Policy Optimization},
  keywords  = {Machine Learning (cs.LG), Optimization and Control (math.OC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  publisher = {arXiv},
  ranking   = {rank4},
}

@Article{Boehn2023,
  author    = {Eivind B{\o}hn and Sebastien Gros and Signe Moe and Tor Arne Johansen},
  journal   = {Engineering Applications of Artificial Intelligence},
  title     = {Optimization of the model predictive control meta-parameters through reinforcement learning},
  year      = {2023},
  month     = {aug},
  pages     = {106211},
  volume    = {123},
  doi       = {10.1016/j.engappai.2023.106211},
  groups    = {Proximal Policy Optimization},
  publisher = {Elsevier {BV}},
  ranking   = {rank3},
}

@Article{Schulman2017,
  author    = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  title     = {Proximal Policy Optimization Algorithms},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1707.06347},
  groups    = {Proximal Policy Optimization},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
  ranking   = {rank3},
}

@Article{Gu2022,
  author    = {Yang Gu and Yuhu Cheng and C. L. Philip Chen and Xuesong Wang},
  journal   = {{IEEE} Transactions on Systems, Man, and Cybernetics: Systems},
  title     = {Proximal Policy Optimization With Policy Feedback},
  year      = {2022},
  month     = {jul},
  number    = {7},
  pages     = {4600--4610},
  volume    = {52},
  doi       = {10.1109/tsmc.2021.3098451},
  groups    = {Proximal Policy Optimization},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  ranking   = {rank2},
}

@Book{Birgin2014,
  author    = {Birgin, Ernesto Julian Goldberg},
  editor    = {José Mario Martínez},
  publisher = {SIAM, Society for Industrial and Applied Mathematics},
  title     = {Practical augmented Lagrangian methods for constrained optimization},
  year      = {2014},
  address   = {Philadelphia},
  isbn      = {9781611973365},
  note      = {Includes bibliographical references (pages 199-214) and indexes},
  number    = {10},
  series    = {Fundamentals of algorithms},
  groups    = {ALM-PANOC},
  pagetotal = {1220},
  ppn_gvk   = {1654871311},
  ranking   = {rank5},
}

@Book{Dong2020,
  editor    = {Hao Dong and Zihan Ding and Shanghang Zhang},
  publisher = {Springer Singapore Pte. Limited},
  title     = {Deep Reinforcement Learning},
  year      = {2020},
  address   = {Singapore},
  isbn      = {9789811540943},
  note      = {Description based on publisher supplied metadata and other sources.},
  groups    = {General},
  pagetotal = {514},
  ppn_gvk   = {1800020597},
  ranking   = {rank3},
  subtitle  = {Fundamentals, Research and Applications},
}

@Article{Arulkumaran2017,
  author        = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  journal       = {{IEEE} Signal Processing Magazine},
  title         = {A Brief Survey of Deep Reinforcement Learning},
  year          = {2017},
  month         = {nov},
  number        = {6},
  pages         = {26--38},
  volume        = {34},
  abstract      = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep $Q$-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  date          = {2017-08-19},
  doi           = {10.1109/msp.2017.2743240},
  eprint        = {1708.05866},
  file          = {:http\://arxiv.org/pdf/1708.05866v2:PDF},
  groups        = {General},
  keywords      = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {Institute of Electrical and Electronics Engineers ({IEEE})},
  ranking       = {rank3},
}

@Book{Li2017,
  author        = {Li, Yuxi},
  publisher     = {arXiv},
  title         = {Deep Reinforcement Learning: An Overview},
  year          = {2017},
  month         = jan,
  abstract      = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions. Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant update.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1701.07274},
  eprint        = {1701.07274},
  file          = {:Li2017 - Deep Reinforcement Learning_ an Overview.pdf:PDF},
  groups        = {General},
  keywords      = {Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  ranking       = {rank3},
}

@Book{Rakovic2019,
  editor    = {Saša V. Raković and William S. Levine},
  publisher = {Springer International Publishing},
  title     = {Handbook of Model Predictive Control},
  year      = {2019},
  address   = {Cham},
  isbn      = {9783319774893},
  series    = {SpringerLink},
  groups    = {General},
  pagetotal = {69215497},
  ppn_gvk   = {1031840370},
  ranking   = {rank4},
}

@Book{Sutton1998,
  author    = {Sutton, Richard S.},
  editor    = {Andrew G. Barto},
  publisher = {MIT Press},
  title     = {Reinforcement learning},
  year      = {1998},
  address   = {Cambridge, Massachusetts},
  isbn      = {9780262257053},
  note      = {Includes bibliographical references (p. [291]-312) and index. - Description based on PDF viewed 12/23/2015},
  series    = {Adaptive computation and machine learning series},
  groups    = {General},
  pagetotal = {1322},
  ppn_gvk   = {1727350383},
  ranking   = {rank4},
  subtitle  = {An introduction},
}

@Article{FrancoisLavet2018,
  author    = {Vincent Fran{\c{c}}ois-Lavet and Peter Henderson and Riashat Islam and Marc G. Bellemare and Joelle Pineau},
  journal   = {Foundations and Trends{\textregistered} in Machine Learning},
  title     = {An Introduction to Deep Reinforcement Learning},
  year      = {2018},
  number    = {3-4},
  pages     = {219--354},
  volume    = {11},
  doi       = {10.1561/2200000071},
  groups    = {General},
  publisher = {Now Publishers},
  ranking   = {rank3},
}

@Article{Bertsekas2005,
  author    = {Dimitri P. Bertsekas},
  journal   = {European Journal of Control},
  title     = {Dynamic Programming and Suboptimal Control: A Survey from {ADP} to {MPC}{\ast}},
  year      = {2005},
  month     = {jan},
  number    = {4-5},
  pages     = {310--334},
  volume    = {11},
  doi       = {10.3166/ejc.11.310-334},
  groups    = {DP-MPC theory},
  publisher = {Elsevier {BV}},
}

@Article{Busoniu2018,
  author    = {Lucian Bu{\c{s}}oniu and Tim de Bruin and Domagoj Toli{\'{c}} and Jens Kober and Ivana Palunko},
  journal   = {Annual Reviews in Control},
  title     = {Reinforcement learning for control: Performance, stability, and deep approximators},
  year      = {2018},
  pages     = {8--28},
  volume    = {46},
  doi       = {10.1016/j.arcontrol.2018.09.005},
  groups    = {DP-MPC theory},
  publisher = {Elsevier {BV}},
}

@Article{Fan2019,
  author        = {Fan, Jianqing and Wang, Zhaoran and Xie, Yuchen and Yang, Zhuoran},
  title         = {A Theoretical Analysis of Deep Q-Learning},
  year          = {2019},
  month         = jan,
  abstract      = {Despite the great empirical success of deep reinforcement learning, its theoretical foundation is less well understood. In this work, we make the first attempt to theoretically understand the deep Q-network (DQN) algorithm (Mnih et al., 2015) from both algorithmic and statistical perspectives. In specific, we focus on a slight simplification of DQN that fully captures its key features. Under mild assumptions, we establish the algorithmic and statistical rates of convergence for the action-value functions of the iterative policy sequence obtained by DQN. In particular, the statistical error characterizes the bias and variance that arise from approximating the action-value function using deep neural network, while the algorithmic error converges to zero at a geometric rate. As a byproduct, our analysis provides justifications for the techniques of experience replay and target network, which are crucial to the empirical success of DQN. Furthermore, as a simple extension of DQN, we propose the Minimax-DQN algorithm for zero-sum Markov game with two players. Borrowing the analysis of DQN, we also quantify the difference between the policies obtained by Minimax-DQN and the Nash equilibrium of the Markov game in terms of both the algorithmic and statistical rates of convergence.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1901.00137},
  eprint        = {1901.00137},
  file          = {:Fan2019 - A Theoretical Analysis of Deep Q Learning.pdf:PDF},
  groups        = {Q-Learning},
  keywords      = {Machine Learning (cs.LG), Optimization and Control (math.OC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Mathematics},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{Sakurai2010,
  author    = {Y Sakurai and K Takada and T Kawabe and S Tsuruta},
  title     = {A Method to Control Parameters of Evolutionary Algorithms by Using Reinforcement Learning},
  year      = {2010},
  month     = {dec},
  booktitle = {2010 Sixth International Conference on Signal-Image Technology and Internet Based Systems},
  doi       = {10.1109/sitis.2010.22},
  groups    = {Q-Learning},
  publisher = {{IEEE}},
}

@Article{Sun2021,
  author        = {Sun, Ke and Wang, Yafei and Liu, Yi and Zhao, Yingnan and Pan, Bo and Jui, Shangling and Jiang, Bei and Kong, Linglong},
  title         = {Damped Anderson Mixing for Deep Reinforcement Learning: Acceleration, Convergence, and Stabilization},
  year          = {2021},
  month         = oct,
  abstract      = {Anderson mixing has been heuristically applied to reinforcement learning (RL) algorithms for accelerating convergence and improving the sampling efficiency of deep RL. Despite its heuristic improvement of convergence, a rigorous mathematical justification for the benefits of Anderson mixing in RL has not yet been put forward. In this paper, we provide deeper insights into a class of acceleration schemes built on Anderson mixing that improve the convergence of deep RL algorithms. Our main results establish a connection between Anderson mixing and quasi-Newton methods and prove that Anderson mixing increases the convergence radius of policy iteration schemes by an extra contraction factor. The key focus of the analysis roots in the fixed-point iteration nature of RL. We further propose a stabilization strategy by introducing a stable regularization term in Anderson mixing and a differentiable, non-expansive MellowMax operator that can allow both faster convergence and more stable behavior. Extensive experiments demonstrate that our proposed method enhances the convergence, stability, and performance of RL algorithms.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2110.08896},
  eprint        = {2110.08896},
  file          = {:Sun2021 - Damped Anderson Mixing for Deep Reinforcement Learning_ Acceleration, Convergence, and Stabilization.pdf:PDF},
  groups        = {Q-Learning},
  keywords      = {Machine Learning (cs.LG), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Mathematics},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{Goldsztejn2023,
  author        = {Goldsztejn, Elias and Feiner, Tal and Brafman, Ronen},
  title         = {PTDRL: Parameter Tuning using Deep Reinforcement Learning},
  year          = {2023},
  month         = jun,
  abstract      = {A variety of autonomous navigation algorithms exist that allow robots to move around in a safe and fast manner. However, many of these algorithms require parameter re-tuning when facing new environments. In this paper, we propose PTDRL, a parameter-tuning strategy that adaptively selects from a fixed set of parameters those that maximize the expected reward for a given navigation system. Our learning strategy can be used for different environments, different platforms, and different user preferences. Specifically, we attend to the problem of social navigation in indoor spaces, using a classical motion planning algorithm as our navigation system and training its parameters to optimize its behavior. Experimental results show that PTDRL can outperform other online parameter-tuning strategies.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2306.10833},
  eprint        = {2306.10833},
  file          = {:Goldsztejn2023 - PTDRL_ Parameter Tuning Using Deep Reinforcement Learning.pdf:PDF},
  groups        = {Q-Learning},
  keywords      = {Robotics (cs.RO), FOS: Computer and information sciences},
  primaryclass  = {cs.RO},
  publisher     = {arXiv},
}

@Article{Goerges2017,
  author    = {Daniel Görges},
  journal   = {{IFAC}-{PapersOnLine}},
  title     = {Relations between Model Predictive Control and Reinforcement Learning},
  year      = {2017},
  month     = {jul},
  number    = {1},
  pages     = {4920--4928},
  volume    = {50},
  doi       = {10.1016/j.ifacol.2017.08.747},
  groups    = {General},
  publisher = {Elsevier {BV}},
}

@Article{Bertsekas2017,
  author    = {Dimitri P. Bertsekas},
  journal   = {{IEEE} Transactions on Neural Networks and Learning Systems},
  title     = {Value and Policy Iterations in Optimal Control and Adaptive Dynamic Programming},
  year      = {2017},
  month     = {mar},
  number    = {3},
  pages     = {500--509},
  volume    = {28},
  doi       = {10.1109/tnnls.2015.2503980},
  groups    = {General},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Book{Bertsekas2016,
  author    = {Bertsekas, Dimitri P.},
  editor    = {John N. Tsitsiklis},
  publisher = {Athena Scientific},
  title     = {Neuro-dynamic programming},
  year      = {2016},
  address   = {Belmont, Massachusetts},
  edition   = {3rd printing},
  isbn      = {9781886529106},
  note      = {Literaturverzeichnis: S. 475-486},
  groups    = {General},
  pagetotal = {491},
  ppn_gvk   = {1750045702},
}

@Book{Bertsekas2005a,
  author    = {Bertsekas, Dimitri P.},
  publisher = {Athena Scientific},
  title     = {Bertsekas},
  year      = {2005},
  address   = {Belmont, Mass.},
  edition   = {3. ed.},
  isbn      = {1886529264},
  volume    = {1},
  groups    = {General},
  pagetotal = {543},
  ppn_gvk   = {1190788519},
  subtitle  = {Dimitri P.},
}

@Misc{Vamvoudakis2021,
  author    = {Vamvoudakis, Kyriakos G.},
  note      = {Description based on publisher supplied metadata and other sources.},
  title     = {Handbook of Reinforcement Learning and Control},
  year      = {2021},
  address   = {Cham},
  editor    = {Yan Wan and Frank L. Lewis and Derya Cansever},
  groups    = {General},
  isbn      = {9783030609900},
  number    = {v.325},
  pagetotal = {1839},
  ppn_gvk   = {1816825751},
  publisher = {Springer International Publishing AG},
  series    = {Studies in Systems, Decision and Control Ser.},
}

@Misc{Wiering2012,
  author    = {Wiering, Marco},
  note      = {Description based on publisher supplied metadata and other sources.},
  title     = {Reinforcement Learning},
  year      = {2012},
  address   = {Berlin, Heidelberg},
  edition   = {1st ed.},
  editor    = {Martijn van Otterlo},
  groups    = {General},
  isbn      = {9783642276453},
  number    = {v.12},
  pagetotal = {1653},
  ppn_gvk   = {1746412844},
  publisher = {Springer Berlin / Heidelberg},
  series    = {Adaptation, Learning, and Optimization Ser.},
  subtitle  = {State-Of-the-Art},
}

@Book{HernandezLerma2023,
  author    = {Hernández-Lerma, Onésimo},
  editor    = {Leonardo R. Laura-Guarachi and Saúl Mendoza-Palacios and David González-Sánchez},
  publisher = {Springer},
  title     = {An introduction to optimal control theory},
  year      = {2023},
  address   = {Cham, Switzerland},
  isbn      = {9783031211386},
  note      = {Literaturverzeichnis: Seite 263-270},
  number    = {volume 76},
  series    = {Texts in applied mathematics},
  groups    = {General},
  pagetotal = {273},
  ppn_gvk   = {1845433157},
  subtitle  = {The dynamic programming approach},
}

@Book{Bertsekas2019,
  author    = {Bertsekas, Dimitri P.},
  publisher = {Athena Scientific},
  title     = {Reinforcement learning and optimal control},
  year      = {2019},
  address   = {Belmont, Massachusetts},
  edition   = {2nd printing (includes editorial revisions)},
  isbn      = {1886529396},
  note      = {Literaturverzeichnis: Seite 345-368},
  groups    = {General},
  pagetotal = {373},
  ppn_gvk   = {1690560037},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:General\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Proximal Policy Optimization\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:ALM-PANOC\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Q-Learning\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:DP-MPC theory\;0\;1\;0x8a8a8aff\;\;\;;
}
